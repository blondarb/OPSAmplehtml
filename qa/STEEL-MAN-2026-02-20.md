# Steel Man Test Report — Note Synthesis & Dedup Pipeline

**Project:** Sevaro Clinical - OPSAmplehtml
**Date:** 2026-02-20
**Tester:** Claude Code (Steel Man Testing Skill)
**Product Type:** Full Stack Web App (Next.js + Supabase)
**Live URL:** https://ops-amplehtml.vercel.app
**Repo:** https://github.com/blondarb/OPSAmplehtml
**Branch/Commit:** main @ de32398
**Scope:** Note synthesis pipeline, deduplication, chart prep reference panel, Visit AI field writing, Generate Note modal, sign workflow

---

## Executive Summary

The three-phase clinical note pipeline (Chart Prep → Visit AI → Note Synthesis) has solid architectural foundations. The merge engine, synthesis API, note review API, and preview modal are all built and connected. However, **the pipeline has never been tested end-to-end with all three data sources populated simultaneously**, and several integration gaps would cause real workflow failures.

The strongest argument against shipping this workflow: **a clinician who uses Chart Prep, records the visit, then opens Generate Note will encounter a sign-from-preview path that silently fails to actually sign the note in the database** (known TODO). More critically, the **chart prep output may not reach the synthesis API** because the data flow from the new reference-only panel hasn't been verified with the synthesis endpoint's expected input shape.

**Overall Readiness:** Ready for Integration Testing — all S0 and S1 bugs fixed, integration testing still needed.

**Findings Summary:**

| Severity | Count | Fixed |
|----------|-------|-------|
| S0 — Ship Blocker | 1 | 1 ✅ |
| S1 — Must Fix Before Launch | 4 | 4 ✅ |
| S2 — Should Fix Soon | 3 | 0 |
| S3 — Nice to Have | 3 | 0 |
| S4 — Observation | 3 | 0 |

---

## S0 — Ship Blocker

### ✅ FIXED: S0-1: Sign from Preview Modal Does Not Actually Sign

**Area:** EnhancedNotePreviewModal → ClinicalNote.tsx
**File:** `src/components/ClinicalNote.tsx`
**Status:** Fixed — `handleSignFromPreview` now delegates to `handleSignComplete()` via ref, which properly calls `handlePend()` → sign API → `resetAllClinicalState()` → follow-up modal.

---

## S1 — Must Fix Before Launch

### ✅ FIXED: S1-1: Chart Prep Data May Not Reach Synthesis API After Reference-Only Refactor

**Area:** Data flow from ClinicalNote → EnhancedNotePreviewModal → synthesize-note API
**Files:** `ClinicalNote.tsx`
**Status:** Confirmed and fixed. The chart-prep API returns `summary` and `alerts` but `ChartPrepOutput` expects `patientSummary` and `keyConsiderations`. Added field mapping in `handleChartPrepComplete` so the synthesis API receives the full narrative summary.

### ✅ FIXED: S1-2: Visit AI Markers Leak Into Synthesis Input

**Area:** Note field content with `--- Visit AI ---` markers sent to synthesis API
**Files:** `EnhancedNotePreviewModal.tsx`
**Status:** Fixed. Added `stripMarkers()` utility in the synthesis call that strips all `--- Visit AI ---`, `--- Chart Prep ---`, and `--- Pre-Visit Notes ---` marker blocks from manual data before sending to the API. This prevents duplicate content and token waste.

### ✅ FIXED: S1-3: No Error Recovery in Synthesis Pipeline

**Area:** EnhancedNotePreviewModal synthesis flow
**File:** `EnhancedNotePreviewModal.tsx`
**Status:** Fixed. Separated synthesis errors from generation errors into a dedicated `synthesisError` state. When synthesis fails, the local merge output remains fully visible and usable. An inline error banner appears at the top of the content area with a "Retry Synthesis" button. The main "AI Synthesize All" button also switches to amber "Retry Synthesis" state. The error is dismissable and does not block save/sign of the local merge.

### ✅ FIXED: S1-4: Note Review Suggestions Reference Section IDs That May Not Match

**Area:** Note review API → suggestion display in modal
**Files:** `note-review/route.ts`
**Status:** Fixed. Added the explicit list of valid section IDs to the system prompt so the AI always returns matching IDs for "Go to section" links.

---

## S2 — Should Fix Soon

### S2-1: Synthesis Model Is gpt-5.2 But Documentation Says gpt-5

**Area:** Documentation consistency
**Files:** `synthesize-note/route.ts:152`, `docs/AI_PROMPTS_AND_MODELS.md`, `IMPLEMENTATION_STATUS.md:633`

**Problem:** The actual code uses `gpt-5.2` but the implementation status doc says `gpt-5`. Minor, but could cause confusion when debugging costs or performance.

**Recommendation:** Update documentation to reflect actual model usage.

### S2-2: Note Review Uses gpt-5-mini with Default Temperature

**Area:** Note review quality
**File:** `note-review/route.ts:79`

**Problem:** Comment says `// Note: gpt-5-mini only supports default temperature (1)` — using temperature 1 for a quality analysis task means the suggestions will be more variable/creative than desired. A clinical documentation review should be deterministic.

**Impact:** Inconsistent review suggestions — running the same note through review twice may produce different findings.

**Recommendation:** If the model supports lower temperature, set it to 0.3. If it truly only supports default, add a note explaining why suggestions may vary and consider switching to a model that supports temperature control.

### S2-3: No Integration Tests for the Synthesis Pipeline

**Area:** Test infrastructure
**Files:** None — no test files exist for the synthesis pipeline

**Problem:** There are zero automated tests for:
- `mergeNoteContent()` with various input combinations
- `buildNoteContext()` output format
- `synthesize-note` API with mock data
- `note-review` API with known good/bad notes
- The full modal flow (open → local merge → synthesis → review → save/sign)

**Impact:** Every change to VoiceDrawer, ClinicalNote, the merge engine, or any API is flying blind. Regressions will be caught only by manual testing.

**Recommendation:** Create at minimum:
1. Unit tests for `mergeNoteContent()` with all input combinations
2. API tests for `synthesize-note` and `note-review` with fixture data
3. A test fixture set with realistic chart prep, visit AI, and manual data

---

## S3 — Nice to Have

### S3-1: Synthesis Prompt Is Neurology-Specific but System Is Generic

**Area:** Prompt portability
**File:** `synthesize-note/route.ts:364`

**Problem:** The system prompt hardcodes "You are a neurology clinical documentation specialist." If this system is ever adapted for other specialties, the prompt will produce neurology-flavored notes for cardiology patients.

**Recommendation:** Make the specialty configurable (user setting or practice-level config).

### S3-2: No Undo After Synthesis Replaces Local Merge

**Area:** UX safety
**File:** `EnhancedNotePreviewModal.tsx`

**Problem:** When synthesis completes, it replaces the local merge output. If the clinician preferred the pre-synthesis content, there's no "undo synthesis" action.

**Recommendation:** Store the pre-synthesis formatted note in a ref and add a "Revert to original" option.

### S3-3: Ask AI Context Could Include Source Labels

**Area:** Ask AI quality
**File:** `EnhancedNotePreviewModal.tsx`

**Problem:** The "Ask AI About This Note" feature sends the full note text but doesn't include information about which sections came from which source (chart prep vs visit AI vs manual). This limits the AI's ability to answer questions like "Is the chart prep data consistent with the visit?"

**Recommendation:** Include source badges as annotations in the context sent to Ask AI.

---

## S4 — Observations

### S4-1: Three Voice Recorder Instances Are a Technical Debt Vector

**Area:** Architecture
**Files:** `VoiceDrawer.tsx`, `AiDrawer.tsx`, `EnhancedNotePreviewModal.tsx`

**Observation:** The implementation status doc acknowledges "Three voice recorder instances" as tech debt. Each has its own MediaRecorder, state, and error handling. As the recording pipeline evolves (e.g., adding real-time transcription), changes must be made in three places.

### S4-2: Marker-Based Content Separation Is Fragile

**Area:** Architecture
**Files:** `VoiceDrawer.tsx`

**Observation:** Using `--- Visit AI ---` / `--- End Visit AI ---` text markers in note fields to separate AI from manual content is a fragile pattern. If a clinician manually types "---" or edits within the marker block, the regex stripping can break. The merge engine's `NoteFieldContent` type with `source` tracking is a better pattern that should eventually replace markers entirely.

### S4-3: The "Build the Note" / Dedup Feature Is Essentially Complete

**Area:** Feature completeness assessment

**Observation:** The user asked about planning the dedup/synthesis feature — it turns out most of it is already built:
- **Local merge engine** (`note-merge/`) runs instantly on modal open
- **AI synthesis** (`synthesize-note`) does the real dedup with GPT-5.2
- **Note review** (`note-review`) auto-triggers after synthesis
- **Preview modal** has section-by-section review, inline editing, source badges, dictation, Ask AI

What's missing isn't the feature itself — it's **integration testing** of the end-to-end flow and fixing the bugs identified above (S0-1, S1-1 through S1-4).

---

## Automated Testing Strategy & Feasibility

### The Challenge

Testing the full note synthesis pipeline requires:
1. **Chart prep data** — from audio dictation → transcription → AI summary
2. **Visit AI data** — from visit audio → transcription with diarization → clinical extraction
3. **Manual data** — clinician-entered content in note fields
4. All three flowing into **Generate Note** → local merge → AI synthesis → review → save/sign

### Feasible Automation Approaches

#### Approach A: API-Level Testing (Highest ROI, No Audio Needed)

Skip the audio entirely. The synthesis pipeline doesn't care where the data came from — it takes JSON input.

**What to build:**
1. **Test fixture files** — realistic JSON data for each source:
   - `fixtures/chart-prep-migraine.json` — chart prep output for a migraine patient
   - `fixtures/visit-ai-migraine.json` — visit AI output with clinical extraction
   - `fixtures/manual-migraine.json` — manually entered note data
   - `fixtures/scales-migraine.json` — PHQ-9, MIDAS scores
   - `fixtures/diagnoses-migraine.json` — differential with ICD-10
2. **API test script** — calls `/api/ai/synthesize-note` with fixture data, validates:
   - Response is valid JSON with all expected section keys
   - No section contains raw markers or source labels
   - No section is empty when input had content for it
   - Note review returns valid suggestions
3. **Merge engine unit tests** — test all combinations:
   - Manual only, chart prep only, visit AI only
   - Manual + chart prep, manual + visit AI, all three
   - Empty fields, conflicting fields, overlapping content

**Estimated effort:** 2-3 hours. Can be run via CLI without a browser.

#### Approach B: Synthetic Audio Pipeline (Medium ROI, Realistic E2E)

Generate WAV/MP3 files from text using a text-to-speech service, then feed them through the real transcription pipeline.

**What to build:**
1. **Chart prep scripts** — text files with realistic clinician dictation:
   - `test-audio/scripts/chart-prep-migraine.txt` — "This is a 34-year-old female presenting for evaluation of chronic migraines. She has a history of episodic migraines since age 16..."
   - `test-audio/scripts/visit-migraine.txt` — simulated doctor-patient dialogue
2. **TTS conversion** — use macOS `say` command or OpenAI TTS API to generate audio:
   - `say -o chart-prep-migraine.aiff -v Samantha "$(cat chart-prep-migraine.txt)"` (macOS built-in)
   - Or `openai audio speech` for higher quality
3. **Upload test** — CLI script that:
   - Authenticates with the app
   - POSTs audio to `/api/ai/transcribe` (chart prep) and `/api/ai/visit-ai`
   - Verifies transcription quality
   - Feeds results into synthesis

**Limitations:**
- macOS `say` produces robotic audio — Deepgram may struggle with medical terminology
- OpenAI TTS is higher quality but costs money per run
- The visit-ai endpoint expects a real doctor-patient conversation with speaker diarization — single-voice TTS won't produce meaningful diarization
- Audio files would be 1-5MB each; storing them in the repo isn't ideal

**Estimated effort:** 4-6 hours including TTS quality iteration.

#### Approach C: Browser Automation E2E (Highest Coverage, Most Complex)

Use Claude for Chrome to walk through the entire clinician workflow.

**What to build:**
1. Navigate to the app → select a patient
2. Open Chart Prep → instead of recording, inject text directly into the transcription field (skip audio)
3. Open Document Visit → inject pre-written transcript
4. Manually fill some fields
5. Click Generate Note → verify local merge output
6. Click "AI Synthesize All" → wait for completion → verify
7. Verify note review suggestions appear
8. Click Save / Sign → verify database state

**Limitations:**
- Requires a running dev server or Vercel preview
- Requires auth (demo login)
- Slow (~2-5 minutes per run)
- Brittle to UI changes

**Estimated effort:** 6-8 hours for robust automation.

### Recommended Testing Plan

**Phase 1 (Now):** Approach A — API fixtures + merge engine tests. Highest ROI, catches the critical integration bugs identified in S1-1 and S1-2.

**Phase 2 (Next session):** Approach B — Generate 3-5 synthetic audio files for common scenarios (migraine new consult, seizure follow-up, MS follow-up). Use macOS `say` for quick generation, OpenAI TTS for higher quality if needed. Store scripts (text) in repo, audio files in gitignored directory.

**Phase 3 (Later):** Approach C — Browser E2E for the full happy path. Worth doing before any real user testing.

### Test Scenarios to Cover

| # | Scenario | Data Sources | Expected Outcome |
|---|----------|-------------|-----------------|
| 1 | Chart prep only | CP | Reference banner shows; synthesis uses CP data |
| 2 | Visit AI only | VA | Fields populated with markers; synthesis uses VA data |
| 3 | Manual only | M | No AI content; synthesis passes through manual |
| 4 | All three sources | CP + VA + M | Synthesis deduplicates; manual preferred; all sources visible in context |
| 5 | Conflicting data | CP says "left side", VA says "right side" | Synthesis resolves conflict (visit > chart prep > manual) |
| 6 | Large input | Long transcript + full chart prep + all scales + imaging | Synthesis doesn't hit token limit; note review still works |
| 7 | Empty synthesis result | API error or timeout | Local merge remains visible; retry available |
| 8 | Sign from preview | After synthesis | Visit status changes to "signed" in database |
| 9 | Chart prep markers stripped | Fields have legacy markers | Markers cleaned before synthesis |
| 10 | Visit AI markers stripped | Fields have VA markers | Markers cleaned before synthesis; manual content preserved |

---

## What's Working Well

1. **Three-layer architecture** (local merge → AI synthesis → AI review) is well-designed. The local merge gives instant feedback while the AI synthesis handles the complex dedup work.

2. **Comprehensive type system** — `NoteFieldContent`, `MergedClinicalNote`, `ComprehensiveNoteData`, `FormattedNote` types create clear contracts between components.

3. **Source tracking** — the source badge system (Manual/Chart Prep/Visit AI/Merged/AI Synthesized) gives clinicians visibility into where content came from.

4. **User customization** — the synthesis API accepts note type, note length, section-specific instructions, terminology preferences, and layout preferences. This is more configurable than most commercial AI scribes.

5. **Defensive synthesis prompt** — the "Do NOT say things like 'no exam documented'" guardrail prevents the AI from judging missing data, which is critical for clinical documentation.

---

## Prioritized Action Plan

### Immediate (S0)
1. **Fix sign-from-preview** — wire `handleSignFromPreview` to call `handleSignComplete` properly (`ClinicalNote.tsx:479-490`)

### Before Launch (S1) — ALL FIXED ✅
1. ~~**Verify chart prep data flow**~~ — ✅ Fixed field mapping in `handleChartPrepComplete`
2. ~~**Strip markers before synthesis**~~ — ✅ Added `stripMarkers()` utility in synthesis call
3. ~~**Add synthesis error recovery**~~ — ✅ Separated `synthesisError` state; inline banner with retry; local merge preserved
4. ~~**Fix note review section IDs**~~ — ✅ Added valid ID list to review prompt

### Soon (S2)
1. **Update model documentation** — gpt-5.2 vs gpt-5 discrepancy
2. **Build API test fixtures** — at minimum 3 scenarios (chart prep only, visit AI only, all sources)
3. **Add merge engine unit tests** — all input combinations

### Backlog (S3-S4)
1. Make synthesis prompt specialty-configurable
2. Add "undo synthesis" action in preview modal
3. Include source labels in Ask AI context
4. Long-term: replace marker-based content separation with structured state tracking

---

## Test Environment

- **Tools Used:** Claude Code (code analysis), file reading, git log
- **Duration:** ~45 minutes (code review and analysis)
- **Note:** No live testing performed in this report — findings are from code review only. Browser-based E2E testing should follow as Phase 3 of the automated testing plan.
